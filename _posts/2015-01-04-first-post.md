---
layout: post
title: Curiosity is all you need.
image: /img/hello_world.jpeg
published: true
date: '2018-08-10'
subtitle: Explanation of 'Curiosity-driven Exploration by Self-supervised Prediction'
---

The Reinforcement Learning basically consists of an agent and an environment. The agent executes an action (or decision) and receives a scalar reward (extrinsic) from the environment.

However, in many applications the reward is extremely sparse or not present alltogether.
For example, the rewards in the robotics fetch environments in OpenAI Gym are sparse rewards since the agent only receives a reward of +1 when the arm reaches the given goal position.
The rewards in the game Montezuma's Revenge are extremely sparse since to receive a reward, the agent has to execute a long string of optimal actions.

In such cases, the reward signal provided to the agent can be augmented by an intrinsic reward, a reward generated by the agents understanding of the environment or it's dynamics. 
Curiosity is one of the reward signals that can serve as an intrinsic reward signal.

> Curiosity - a strong desire to know or learn something.

Curiosity in the case of the agent is stated as "the error in an agent's ability to predict the consequence of it's own actions in a visual feature space learned by a self supervised inverse dynamics model".
That is a mouthful. Let's break it down.

Suppose, you have an agent exploring the game of montezuma revenge. In a traditional epsilon greedy exploration strategy, since the actions are completely random, we can reach previously visited states often. 
Now suppose you have a forward dynamics model ie a model that is able to predict the next state given the current state and action. What the paper states is that if you are uncertain about the next state that means that you "learned something". Therefore, the prediction error between the predicted next state and the real next state acts as the intrinsic reward provided to the agent.

However, there is a problem with directly comparing visual states. The environment may have many entities which move around irrespective of the action executed by the agent. For example, the skull in Montezuma's Revenge moves back and forth irrespective of the agent's actions. If we were to naively compare the pixel predictions of the next predicted state and the actual next state, we could always label the next state as curious due to the changes in the environment.

The authors try to solve this problem by using an inverse dynamics model that transforms the raw sensory input to a feature space where only the information relevant to the action executed is represented. This way, the feature space will not contain any information about environmental changes. 
How this is done ?
Simply by using an encoder like architecture that compresses the states into a feature space and then a fully connected network that takes these feature space vectors of the current state and the next state as input and tries to predict the action executed.

The model used is described below 
![Screen Shot 2018-08-13 at 08.06.44.png]({{site.baseurl}}/img/Screen Shot 2018-08-13 at 08.06.44.png)





[ Curiosity-driven Exploration by Self-supervised Prediction](https://arxiv.org/abs/1705.05363)


